# üìä PROYECTO DE SEGMENTACI√ìN DE CLIENTES CON K-MEANS

## Sistema de Clustering No Supervisado para Marketing Estrat√©gico
 ## Nombre Estudiante : Carlos Luis Garcia Lopez 
 ## Curso : Ingenieria Informatica : 3ro 
---

## üìë √çNDICE

1. [Descripci√≥n del Proyecto](#-descripci√≥n-del-proyecto)
2. [Objetivos](#-objetivos)
3. [Tecnolog√≠as y Herramientas](#%EF%B8%8F-tecnolog√≠as-y-herramientas)
4. [Estructura del Proyecto](#-estructura-del-proyecto)
5. [Requisitos Previos](#%EF%B8%8F-requisitos-previos)
6. [Instalaci√≥n y Configuraci√≥n](#-instalaci√≥n-y-configuraci√≥n)
7. [Dataset](#-dataset)
8. [Metodolog√≠a](#-metodolog√≠a)
9. [Gu√≠a de Ejecuci√≥n Paso a Paso](#-gu√≠a-de-ejecuci√≥n-paso-a-paso)
10. [Interpretaci√≥n de Resultados](#-interpretaci√≥n-de-resultados)
11. [Troubleshooting](#-troubleshooting)
12. [Recursos Adicionales](#-recursos-adicionales)

---

## üéØ DESCRIPCI√ìN DEL PROYECTO

Este proyecto implementa un **sistema de segmentaci√≥n de clientes** utilizando el algoritmo de **Machine Learning no supervisado K-Means**. El objetivo es identificar grupos (clusters) de clientes con caracter√≠sticas similares para desarrollar estrategias de marketing personalizadas.

### ¬øQu√© es Machine Learning No Supervisado?

El aprendizaje no supervisado es una t√©cnica donde el algoritmo aprende patrones en los datos **sin etiquetas previas**. A diferencia del aprendizaje supervisado (donde le decimos al algoritmo "esto es un perro, esto es un gato"), aqu√≠ le damos los datos y √©l encuentra grupos por s√≠ mismo.

**Analog√≠a pr√°ctica**: Es como organizar tu inventario de L'Luis sin saber de antemano qu√© categor√≠as crear. El algoritmo mira las caracter√≠sticas de los productos (precio, tama√±o, color, etc.) y autom√°ticamente los agrupa en categor√≠as l√≥gicas.

### ¬øQu√© es K-Means?

**K-Means** es el algoritmo de clustering m√°s popular. Funciona de esta manera:

1. **Eliges K** (el n√∫mero de grupos que quieres crear)
2. El algoritmo coloca K "centroides" (puntos centrales) al azar
3. Asigna cada cliente al centroide m√°s cercano
4. Recalcula la posici√≥n de cada centroide como el promedio de sus clientes
5. Repite los pasos 3-4 hasta que los centroides no se muevan

**Resultado**: Clientes agrupados por similitud en comportamiento, ingresos, gastos, etc.

---

## üéØ OBJETIVOS

### Objetivos del Proyecto

- ‚úÖ **Segmentar clientes** en grupos homog√©neos basados en comportamiento de compra
- ‚úÖ **Identificar perfiles** de clientes (VIP, conservadores, j√≥venes gastadores, etc.)
- ‚úÖ **Desarrollar estrategias** de marketing personalizadas por segmento
- ‚úÖ **Visualizar resultados** con gr√°ficos profesionales en 2D y 3D
- ‚úÖ **Generar reportes** automatizados con insights accionables

### Objetivos de Aprendizaje

- üìö Dominar el algoritmo K-Means desde cero
- üìö Aprender exploraci√≥n y visualizaci√≥n de datos (EDA)
- üìö Implementar preprocesamiento: escalado y normalizaci√≥n
- üìö Determinar el n√∫mero √≥ptimo de clusters (M√©todo del Codo)
- üìö Interpretar resultados desde perspectiva de negocio
- üìö Crear un proyecto completo de ML end-to-end

---

## üõ†Ô∏è TECNOLOG√çAS Y HERRAMIENTAS

### Lenguaje de Programaci√≥n
- **Python 3.8+** (recomendado: Python 3.10 o superior)

### Librer√≠as Principales

| Librer√≠a | Versi√≥n | Prop√≥sito |
|----------|---------|-----------|
| **pandas** | 2.0+ | Manipulaci√≥n y an√°lisis de datos |
| **numpy** | 1.24+ | Operaciones num√©ricas y matrices |
| **matplotlib** | 3.7+ | Visualizaci√≥n de datos b√°sica |
| **seaborn** | 0.12+ | Visualizaci√≥n estad√≠stica avanzada |
| **scikit-learn** | 1.3+ | Algoritmos de Machine Learning |
| **plotly** | 5.14+ | Gr√°ficos interactivos (opcional) |

### Entorno de Desarrollo

**Opci√≥n 1: Jupyter Notebook** ‚≠ê RECOMENDADO PARA EMPEZAR
- Ideal para exploraci√≥n interactiva
- Permite ejecutar c√≥digo celda por celda
- Visualiza resultados inline

**Opci√≥n 2: Python Script (.py)**
- Ideal para producci√≥n y automatizaci√≥n
- M√°s f√°cil de versionar con Git
- Ejecutable desde terminal

**Opci√≥n 3: IDE (PyCharm, VSCode)**
- Entorno completo de desarrollo
- Debugging avanzado
- Autocompletado y sugerencias

### Herramientas Adicionales
- **Git** (control de versiones)
- **Anaconda** o **Miniconda** (gesti√≥n de entornos - opcional)

---

## üìÅ ESTRUCTURA DEL PROYECTO

```
proyecto_segmentacion_clientes/
‚îÇ
‚îú‚îÄ‚îÄ README.md                          # Este archivo - documentaci√≥n principal
‚îú‚îÄ‚îÄ requirements.txt                   # Dependencias del proyecto
‚îú‚îÄ‚îÄ .gitignore                         # Archivos a ignorar en Git
‚îÇ
‚îú‚îÄ‚îÄ data/                              # üìä DATOS
‚îÇ   ‚îú‚îÄ‚îÄ raw/                          # Datos originales sin procesar
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Mall_Customers.csv        # Dataset descargado de Kaggle
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ processed/                    # Datos procesados
‚îÇ       ‚îî‚îÄ‚îÄ clientes_segmentados.csv  # Output con clusters asignados
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                         # üìì JUPYTER NOTEBOOKS
‚îÇ   ‚îú‚îÄ‚îÄ 01_exploracion_datos.ipynb    # An√°lisis exploratorio (EDA)
‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocesamiento.ipynb     # Limpieza y transformaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ 03_modelado.ipynb             # Implementaci√≥n K-Means
‚îÇ   ‚îî‚îÄ‚îÄ 04_visualizacion.ipynb        # Gr√°ficos y reportes
‚îÇ
‚îú‚îÄ‚îÄ src/                               # üíª C√ìDIGO FUENTE
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                     # Configuraci√≥n del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py                # Carga de datos
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py              # Funciones de preprocesamiento
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py                 # Implementaci√≥n K-Means
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py              # Funciones de visualizaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ main.py                       # Script principal ejecutable
‚îÇ
‚îú‚îÄ‚îÄ results/                           # üìà RESULTADOS
‚îÇ   ‚îú‚îÄ‚îÄ figures/                      # Gr√°ficos generados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_exploracion/          # EDA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_clustering/           # Visualizaciones de clusters
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 03_business/             # Gr√°ficos para reportes
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ reports/                      # Reportes generados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reporte_tecnico.txt      # M√©tricas y estad√≠sticas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reporte_negocio.pdf      # Presentaci√≥n ejecutiva
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ models/                       # Modelos guardados
‚îÇ       ‚îî‚îÄ‚îÄ kmeans_final.pkl         # Modelo K-Means entrenado
‚îÇ
‚îú‚îÄ‚îÄ docs/                              # üìö DOCUMENTACI√ìN ADICIONAL
‚îÇ   ‚îú‚îÄ‚îÄ teoria_kmeans.md             # Teor√≠a del algoritmo
‚îÇ   ‚îú‚îÄ‚îÄ interpretacion_negocio.md    # Gu√≠a de interpretaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ guia_aplicacion_lluis.md     # C√≥mo aplicarlo a L'Luis
‚îÇ
‚îî‚îÄ‚îÄ tests/                             # üß™ PRUEBAS (opcional)
    ‚îî‚îÄ‚îÄ test_clustering.py            # Tests unitarios
```

---

## ‚öôÔ∏è REQUISITOS PREVIOS

### Conocimientos Necesarios

#### üü¢ Nivel B√°sico (IMPRESCINDIBLE)
- Python b√°sico (variables, loops, funciones)
- Comprensi√≥n de lectura de CSV
- Uso b√°sico de terminal/l√≠nea de comandos

#### üü° Nivel Intermedio (RECOMENDADO)
- Pandas b√°sico (DataFrames)
- Conceptos de estad√≠stica (media, desviaci√≥n est√°ndar)
- Gr√°ficos con Matplotlib

#### üî¥ Nivel Avanzado (OPCIONAL)
- √Ålgebra lineal (vectores, matrices)
- Optimizaci√≥n matem√°tica
- Git y GitHub

### Software Necesario

1. **Python 3.8+**
   - Descargar: https://www.python.org/downloads/
   - Verificar instalaci√≥n: `python --version`

2. **pip** (gestor de paquetes)
   - Incluido con Python
   - Verificar: `pip --version`

3. **Editor de c√≥digo** (elige uno):
   - Jupyter Notebook (recomendado para empezar)
   - Visual Studio Code
   - PyCharm Community Edition

### Hardware Recomendado

- **RAM**: M√≠nimo 4GB (recomendado 8GB)
- **Espacio**: ~500MB para librer√≠as + datos
- **Procesador**: Cualquier procesador moderno (2+ cores)

> ‚ö†Ô∏è **Nota**: Este proyecto NO requiere GPU. K-Means es computacionalmente ligero.

---

## üîß INSTALACI√ìN Y CONFIGURACI√ìN

### Paso 1: Clonar o Descargar el Proyecto

**Opci√≥n A: Con Git**
```bash
git clone https://github.com/tu-usuario/proyecto-segmentacion-clientes.git
cd proyecto-segmentacion-clientes
```

**Opci√≥n B: Descarga Manual**
1. Descarga el ZIP del proyecto
2. Extrae en la carpeta deseada
3. Abre terminal en esa carpeta

### Paso 2: Crear Entorno Virtual (RECOMENDADO)

**¬øPor qu√© usar entorno virtual?**
- A√≠sla las dependencias del proyecto
- Evita conflictos con otras instalaciones
- Facilita la reproducibilidad

**En Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**En Mac/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**Verificar activaci√≥n:**
Deber√≠as ver `(venv)` al inicio de tu l√≠nea de comandos.

### Paso 3: Instalar Dependencias

**Opci√≥n A: Desde requirements.txt** ‚≠ê RECOMENDADO
```bash
pip install -r requirements.txt
```

**Contenido del requirements.txt:**
```
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
plotly>=5.14.0
jupyter>=1.0.0
ipykernel>=6.0.0
```

**Opci√≥n B: Instalaci√≥n Manual**
```bash
pip install pandas numpy matplotlib seaborn scikit-learn plotly jupyter
```

**Verificar instalaci√≥n:**
```bash
python -c "import pandas, numpy, sklearn, matplotlib; print('‚úÖ Todo instalado correctamente')"
```

### Paso 4: Configurar Jupyter Notebook (si lo usas)

```bash
# Instalar Jupyter
pip install jupyter

# Registrar el entorno virtual como kernel
python -m ipykernel install --user --name=venv --display-name "Python (Segmentaci√≥n)"

# Iniciar Jupyter
jupyter notebook
```

### Paso 5: Crear Estructura de Carpetas

```bash
# En Windows
mkdir data\raw data\processed results\figures\01_exploracion results\figures\02_clustering results\figures\03_business results\reports results\models src notebooks docs tests

# En Mac/Linux
mkdir -p data/raw data/processed results/figures/01_exploracion results/figures/02_clustering results/figures/03_business results/reports results/models src notebooks docs tests
```

---

## üìä DATASET

### Descripci√≥n del Dataset: "Mall Customer Segmentation"

**Fuente**: Kaggle
**Nombre**: Mall Customer Segmentation Data
**URL**: https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python

### Caracter√≠sticas del Dataset

| Atributo | Descripci√≥n | Tipo | Rango |
|----------|-------------|------|-------|
| **CustomerID** | Identificador √∫nico del cliente | Integer | 1-200 |
| **Gender** | G√©nero del cliente | Categ√≥rica | Male / Female |
| **Age** | Edad en a√±os | Integer | 18-70 |
| **Annual Income (k$)** | Ingresos anuales en miles de d√≥lares | Integer | 15-137 |
| **Spending Score (1-100)** | Puntuaci√≥n de gastos asignada por el mall | Integer | 1-99 |

**Total de registros**: 200 clientes
**Valores nulos**: 0 (dataset limpio)

### ¬øC√≥mo Descargar el Dataset?

#### M√©todo 1: Descarga Manual desde Kaggle

1. **Crear cuenta en Kaggle** (gratis)
   - Ve a https://www.kaggle.com
   - Click en "Register" (Registrarse)
   - Completa el formulario

2. **Buscar el dataset**
   - En la barra de b√∫squeda: "Mall Customer Segmentation"
   - O usa el link directo: https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python

3. **Descargar**
   - Click en el bot√≥n "Download" (azul, arriba a la derecha)
   - Se descargar√° `archive.zip`

4. **Extraer y colocar**
   - Descomprime el archivo ZIP
   - Copia `Mall_Customers.csv` a `data/raw/`

#### M√©todo 2: Descarga con Kaggle API (Avanzado)

```bash
# Instalar Kaggle API
pip install kaggle

# Configurar credenciales (ver https://www.kaggle.com/docs/api)
# Descargar dataset
kaggle datasets download -d vjchoudhary7/customer-segmentation-tutorial-in-python

# Mover a carpeta correcta
unzip customer-segmentation-tutorial-in-python.zip -d data/raw/
```

#### M√©todo 3: Dataset Sint√©tico (Sin Kaggle)

Si no puedes usar Kaggle, puedes generar un dataset sint√©tico similar:

```python
import pandas as pd
import numpy as np

np.random.seed(42)

# Generar datos sint√©ticos
n_customers = 200

data = {
    'CustomerID': range(1, n_customers + 1),
    'Gender': np.random.choice(['Male', 'Female'], n_customers),
    'Age': np.random.randint(18, 70, n_customers),
    'Annual Income (k$)': np.random.randint(15, 140, n_customers),
    'Spending Score (1-100)': np.random.randint(1, 100, n_customers)
}

df = pd.DataFrame(data)
df.to_csv('data/raw/Mall_Customers.csv', index=False)
print("‚úÖ Dataset sint√©tico creado")
```

### Verificar que el Dataset est√° Correcto

```python
import pandas as pd

df = pd.read_csv('data/raw/Mall_Customers.csv')

# Verificaciones
assert len(df) == 200, "Debe tener 200 filas"
assert list(df.columns) == ['CustomerID', 'Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)'], "Columnas incorrectas"
assert df.isnull().sum().sum() == 0, "No debe tener valores nulos"

print("‚úÖ Dataset verificado correctamente")
print(df.head())
```

---

## üî¨ METODOLOG√çA

### Proceso End-to-End del Proyecto

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PROCESO DE CLUSTERING                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. EXPLORACI√ìN DE DATOS (EDA)
   ‚Üì
   - Cargar dataset
   - Estad√≠sticas descriptivas
   - Visualizaciones univariadas
   - An√°lisis bivariado
   - Detecci√≥n de outliers
   - An√°lisis de correlaciones

2. PREPROCESAMIENTO
   ‚Üì
   - Manejo de valores nulos (si hay)
   - Codificaci√≥n de variables categ√≥ricas
   - Selecci√≥n de features relevantes
   - Escalado/Normalizaci√≥n de datos
   - Creaci√≥n de subsets

3. DETERMINACI√ìN DE K √ìPTIMO
   ‚Üì
   - M√©todo del Codo (Elbow Method)
   - Silhouette Score (opcional)
   - Davies-Bouldin Index (opcional)

4. ENTRENAMIENTO DEL MODELO
   ‚Üì
   - Inicializar K-Means con K √≥ptimo
   - Ajustar modelo a datos escalados
   - Obtener labels de clusters
   - Extraer centroides

5. EVALUACI√ìN
   ‚Üì
   - Calcular m√©tricas de clustering
   - Analizar distribuci√≥n de clusters
   - Validar coherencia de grupos

6. VISUALIZACI√ìN
   ‚Üì
   - Scatter plots 2D
   - Gr√°ficos 3D (opcional)
   - Heatmaps de caracter√≠sticas
   - Gr√°ficos de barras por segmento

7. INTERPRETACI√ìN DE NEGOCIO
   ‚Üì
   - Perfilar cada cluster
   - Asignar nombres descriptivos
   - Definir estrategias de marketing
   - Generar reportes ejecutivos

8. EXPORTACI√ìN Y DEPLOYMENT
   ‚Üì
   - Guardar modelo entrenado
   - Exportar datos con clusters
   - Crear dashboards (opcional)
   - Documentar hallazgos
```

### Algoritmo K-Means en Detalle

#### Pseudoc√≥digo Simplificado

```
FUNCI√ìN K-Means(datos, K):
    1. Seleccionar K centroides iniciales al azar
    
    2. REPETIR hasta convergencia:
        a. Asignar cada punto al centroide m√°s cercano
           (calcular distancia euclidiana)
        
        b. Recalcular cada centroide como el promedio
           de todos los puntos asignados a √©l
        
        c. SI los centroides no cambiaron:
              SALIR del loop (convergencia)
    
    3. DEVOLVER clusters y centroides finales
FIN FUNCI√ìN
```

#### F√≥rmulas Matem√°ticas Clave

**1. Distancia Euclidiana** (para asignar puntos a clusters)

```
d(p, c) = ‚àö[(x‚ÇÅ - c‚ÇÅ)¬≤ + (x‚ÇÇ - c‚ÇÇ)¬≤ + ... + (x‚Çô - c‚Çô)¬≤]
```

Donde:
- `p` = punto de datos
- `c` = centroide
- `n` = n√∫mero de dimensiones

**2. Actualizaci√≥n de Centroides**

```
c_nuevo = (1/n) √ó Œ£(puntos_en_cluster)
```

Donde:
- `n` = n√∫mero de puntos en el cluster
- `Œ£` = suma de todos los puntos

**3. Inercia (WCSS - Within-Cluster Sum of Squares)**

```
WCSS = Œ£[k=1 to K] Œ£[x en Cluster_k] ||x - Œº‚Çñ||¬≤
```

Donde:
- `K` = n√∫mero de clusters
- `x` = punto de datos
- `Œº‚Çñ` = centroide del cluster k
- `|| ||` = norma euclidiana

---

## üöÄ GU√çA DE EJECUCI√ìN PASO A PASO

### FASE 1: EXPLORACI√ìN DE DATOS (EDA)

**Objetivo**: Entender qu√© datos tenemos antes de aplicar el modelo.

**Archivo**: `notebooks/01_exploracion_datos.ipynb` o crear script Python.

#### Paso 1.1: Cargar y Visualizar Datos

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Configuraci√≥n
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

# Cargar datos
df = pd.read_csv('data/raw/Mall_Customers.csv')

# Primeras filas
print("‚ïê" * 80)
print("PRIMERAS 5 FILAS DEL DATASET")
print("‚ïê" * 80)
print(df.head())

# Informaci√≥n del DataFrame
print("\n" + "‚ïê" * 80)
print("INFORMACI√ìN DEL DATASET")
print("‚ïê" * 80)
print(df.info())

# Estad√≠sticas descriptivas
print("\n" + "‚ïê" * 80)
print("ESTAD√çSTICAS DESCRIPTIVAS")
print("‚ïê" * 80)
print(df.describe())
```

**¬øQu√© buscar?**
- ‚úÖ N√∫mero de filas y columnas
- ‚úÖ Tipos de datos correctos
- ‚úÖ Valores nulos (deber√≠an ser 0)
- ‚úÖ Rangos de valores l√≥gicos

#### Paso 1.2: An√°lisis Univariado

```python
# Crear figura con subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('AN√ÅLISIS UNIVARIADO', fontsize=16, fontweight='bold')

# Distribuci√≥n de Edad
axes[0, 0].hist(df['Age'], bins=20, color='skyblue', edgecolor='black')
axes[0, 0].set_title('Distribuci√≥n de Edades')
axes[0, 0].set_xlabel('Edad')
axes[0, 0].set_ylabel('Frecuencia')
axes[0, 0].axvline(df['Age'].mean(), color='red', linestyle='--', 
                    label=f'Media: {df["Age"].mean():.1f}')
axes[0, 0].legend()

# Distribuci√≥n de Ingresos
axes[0, 1].hist(df['Annual Income (k$)'], bins=20, color='lightgreen', edgecolor='black')
axes[0, 1].set_title('Distribuci√≥n de Ingresos')
axes[0, 1].set_xlabel('Ingresos Anuales (k$)')
axes[0, 1].set_ylabel('Frecuencia')
axes[0, 1].axvline(df['Annual Income (k$)'].mean(), color='red', linestyle='--',
                    label=f'Media: {df["Annual Income (k$)"].mean():.1f}')
axes[0, 1].legend()

# Distribuci√≥n de Spending Score
axes[1, 0].hist(df['Spending Score (1-100)'], bins=20, color='salmon', edgecolor='black')
axes[1, 0].set_title('Distribuci√≥n de Spending Score')
axes[1, 0].set_xlabel('Spending Score')
axes[1, 0].set_ylabel('Frecuencia')
axes[1, 0].axvline(df['Spending Score (1-100)'].mean(), color='red', linestyle='--',
                    label=f'Media: {df["Spending Score (1-100)"].mean():.1f}')
axes[1, 0].legend()

# Distribuci√≥n de G√©nero
gender_counts = df['Gender'].value_counts()
axes[1, 1].bar(gender_counts.index, gender_counts.values, 
               color=['lightblue', 'pink'], edgecolor='black')
axes[1, 1].set_title('Distribuci√≥n de G√©nero')
axes[1, 1].set_ylabel('Frecuencia')
for i, v in enumerate(gender_counts.values):
    axes[1, 1].text(i, v + 2, str(v), ha='center', fontweight='bold')

plt.tight_layout()
plt.savefig('results/figures/01_exploracion/univariado.png', dpi=300)
plt.show()
```

**Interpretaci√≥n esperada**:
- **Edad**: Distribuci√≥n variada, posiblemente bimodal
- **Ingresos**: Rango amplio (15k-137k$)
- **Spending Score**: Distribuci√≥n uniforme
- **G√©nero**: Aproximadamente balanceado

#### Paso 1.3: An√°lisis Bivariado (MUY IMPORTANTE)

```python
# Scatter plots para identificar patrones
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
fig.suptitle('AN√ÅLISIS BIVARIADO - B√∫squeda de Patrones', fontsize=16, fontweight='bold')

# Edad vs Spending Score
axes[0].scatter(df['Age'], df['Spending Score (1-100)'], 
                c=df['Gender'].map({'Male': 'blue', 'Female': 'red'}),
                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)
axes[0].set_xlabel('Edad')
axes[0].set_ylabel('Spending Score')
axes[0].set_title('Edad vs Spending Score')
axes[0].legend(['Hombres', 'Mujeres'], loc='best')
axes[0].grid(True, alpha=0.3)

# Ingresos vs Spending Score (LA M√ÅS IMPORTANTE)
axes[1].scatter(df['Annual Income (k$)'], df['Spending Score (1-100)'],
                c=df['Gender'].map({'Male': 'blue', 'Female': 'red'}),
                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)
axes[1].set_xlabel('Ingresos Anuales (k$)')
axes[1].set_ylabel('Spending Score')
axes[1].set_title('Ingresos vs Spending Score ‚≠ê')
axes[1].grid(True, alpha=0.3)

# Edad vs Ingresos
axes[2].scatter(df['Age'], df['Annual Income (k$)'],
                c=df['Gender'].map({'Male': 'blue', 'Female': 'red'}),
                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)
axes[2].set_xlabel('Edad')
axes[2].set_ylabel('Ingresos Anuales (k$)')
axes[2].set_title('Edad vs Ingresos')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results/figures/01_exploracion/bivariado.png', dpi=300)
plt.show()
```

**¬øQu√© buscamos?**
- üëÄ **Clusters visibles**: ¬øSe ven grupos naturales?
- üëÄ **Patrones**: ¬øHay correlaciones?
- üëÄ **Outliers**: ¬øPuntos muy alejados?

**HALLAZGO CLAVE**: En el gr√°fico Ingresos vs Spending Score deber√≠as ver ~5 grupos naturales.

#### Paso 1.4: Matriz de Correlaci√≥n

```python
# Seleccionar solo columnas num√©ricas
numerical_df = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]

# Calcular correlaci√≥n
correlation_matrix = numerical_df.corr()

# Visualizar con heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', 
            center=0, square=True, linewidths=2, cbar_kws={"shrink": 0.8},
            fmt='.2f', vmin=-1, vmax=1)
plt.title('MATRIZ DE CORRELACI√ìN', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('results/figures/01_exploracion/correlacion.png', dpi=300)
plt.show()

print("\nCOEFICIENTES DE CORRELACI√ìN:")
print(correlation_matrix)
```

**Interpretaci√≥n**:
- Valores cercanos a **+1**: Correlaci√≥n positiva fuerte
- Valores cercanos a **-1**: Correlaci√≥n negativa fuerte
- Valores cercanos a **0**: No hay correlaci√≥n

---

### FASE 2: PREPROCESAMIENTO

**Objetivo**: Preparar los datos para el algoritmo K-Means.

**Archivo**: `src/preprocessing.py` o continuar en notebook.

#### Paso 2.1: Codificar Variables Categ√≥ricas

```python
from sklearn.preprocessing import LabelEncoder

# Crear copia para no modificar original
df_processed = df.copy()

# Codificar g√©nero
label_encoder = LabelEncoder()
df_processed['Gender_Encoded'] = label_encoder.fit_transform(df['Gender'])

print("CODIFICACI√ìN DE G√âNERO:")
print(df_processed[['Gender', 'Gender_Encoded']].drop_duplicates())
```

**Resultado esperado**:
```
  Gender  Gender_Encoded
0 Female               0
1   Male               1
```

#### Paso 2.2: Selecci√≥n de Features

Vamos a crear **DOS conjuntos de features**:

```python
# CONJUNTO 1: Solo Ingresos y Spending Score (2D - m√°s f√°cil de visualizar)
X_simple = df[['Annual Income (k$)', 'Spending Score (1-100)']].values
print(f"X_simple shape: {X_simple.shape}")  # Debe ser (200, 2)

# CONJUNTO 2: Todas las features (multidimensional - m√°s completo)
X_completo = df_processed[['Age', 'Gender_Encoded', 
                            'Annual Income (k$)', 
                            'Spending Score (1-100)']].values
print(f"X_completo shape: {X_completo.shape}")  # Debe ser (200, 4)
```

**¬øCu√°l usar?**
- **X_simple**: Para aprender y visualizar f√°cilmente
- **X_completo**: Para an√°lisis m√°s robusto (recomendado en producci√≥n)

#### Paso 2.3: Escalado de Datos (CR√çTICO)

**¬øPor qu√© escalar?**
K-Means usa distancia euclidiana. Si una variable va de 0-100 y otra de 0-150000, la segunda dominar√° el c√°lculo.

```python
from sklearn.preprocessing import StandardScaler

# Crear escaladores
scaler_simple = StandardScaler()
scaler_completo = StandardScaler()

# Escalar
X_simple_scaled = scaler_simple.fit_transform(X_simple)
X_completo_scaled = scaler_completo.fit_transform(X_completo)

# Comparar antes y despu√©s
print("‚ïê" * 80)
print("DATOS ORIGINALES (primeras 3 filas):")
print("‚ïê" * 80)
print(X_simple[:3])

print("\n" + "‚ïê" * 80)
print("DATOS ESCALADOS (primeras 3 filas):")
print("‚ïê" * 80)
print(X_simple_scaled[:3])

print("\n" + "‚ïê" * 80)
print("ESTAD√çSTICAS DATOS ESCALADOS:")
print("‚ïê" * 80)
print(f"Media: {X_simple_scaled.mean(axis=0)}")  # Debe ser ~[0, 0]
print(f"Desviaci√≥n est√°ndar: {X_simple_scaled.std(axis=0)}")  # Debe ser ~[1, 1]
```

**F√≥rmula de StandardScaler**:
```
z = (x - Œº) / œÉ
```
Donde:
- `x` = valor original
- `Œº` = media de la columna
- `œÉ` = desviaci√≥n est√°ndar de la columna
- `z` = valor escalado

---

### FASE 3: DETERMINACI√ìN DE K √ìPTIMO

**Objetivo**: Encontrar el n√∫mero ideal de clusters usando el **M√©todo del Codo**.

#### Paso 3.1: M√©todo del Codo (Elbow Method)

```python
from sklearn.cluster import KMeans

# Probar diferentes valores de K
inertias = []
K_range = range(1, 11)

print("CALCULANDO INERCIA PARA DIFERENTES VALORES DE K...")
print("‚ïê" * 60)

for k in K_range:
    kmeans = KMeans(n_clusters=k, 
                    random_state=42,      # Para reproducibilidad
                    n_init=10,            # N√∫mero de inicializaciones
                    max_iter=300)         # M√°ximo de iteraciones
    
    kmeans.fit(X_simple_scaled)
    inertias.append(kmeans.inertia_)
    
    print(f"K = {k:2d} ‚Üí Inercia = {kmeans.inertia_:8.2f}")

print("‚ïê" * 60)
```

#### Paso 3.2: Graficar el Codo

```python
plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=10)
plt.xlabel('N√∫mero de Clusters (K)', fontsize=12, fontweight='bold')
plt.ylabel('Inercia (WCSS)', fontsize=12, fontweight='bold')
plt.title('M√âTODO DEL CODO - Determinaci√≥n de K √ìptimo', 
          fontsize=14, fontweight='bold', pad=20)
plt.grid(True, alpha=0.3)
plt.xticks(K_range)

# Marcar K=5 si es el √≥ptimo
plt.axvline(x=5, color='red', linestyle='--', alpha=0.5, label='K √≥ptimo sugerido')
plt.legend()

plt.tight_layout()
plt.savefig('results/figures/02_clustering/metodo_codo.png', dpi=300)
plt.show()
```

**¬øC√≥mo interpretar el gr√°fico?**

```
Inercia
   ‚îÇ
   ‚îÇ *
   ‚îÇ   *
   ‚îÇ     *        ‚Üê CODO (punto √≥ptimo)
   ‚îÇ       *___
   ‚îÇ           *___*___*___
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ K
     1  2  3  4  5  6  7  8  9
```

- El "codo" es donde la curva cambia de pendiente pronunciada a suave
- **Antes del codo**: Mucha mejora al a√±adir clusters
- **Despu√©s del codo**: Poca mejora (no vale la pena la complejidad)

**Para este dataset**, K=5 suele ser √≥ptimo.

#### Paso 3.3: Silhouette Score (M√©trica Complementaria - Opcional)

```python
from sklearn.metrics import silhouette_score

silhouette_scores = []

for k in range(2, 11):  # Silhouette necesita K >= 2
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_simple_scaled)
    score = silhouette_score(X_simple_scaled, labels)
    silhouette_scores.append(score)
    print(f"K = {k} ‚Üí Silhouette Score = {score:.4f}")

# Graficar
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, 'go-', linewidth=2, markersize=10)
plt.xlabel('N√∫mero de Clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score por K')
plt.grid(True, alpha=0.3)
plt.savefig('results/figures/02_clustering/silhouette.png', dpi=300)
plt.show()
```

**Interpretaci√≥n Silhouette Score**:
- Rango: **-1 a +1**
- **Cerca de +1**: Clusters bien definidos
- **Cerca de 0**: Clusters solapados
- **Negativo**: Puntos mal asignados

El K con **mayor Silhouette Score** es otra buena opci√≥n.

---

### FASE 4: ENTRENAMIENTO DEL MODELO

**Objetivo**: Crear y entrenar el modelo K-Means con K √≥ptimo.

#### Paso 4.1: Crear y Entrenar Modelo

```python
# Definir K √≥ptimo (ajusta seg√∫n tu an√°lisis)
K_OPTIMO = 5

print("‚ïê" * 80)
print(f"ENTRENANDO K-MEANS CON K = {K_OPTIMO}")
print("‚ïê" * 80)

# Crear modelo
kmeans_final = KMeans(
    n_clusters=K_OPTIMO,
    random_state=42,        # Semilla para reproducibilidad
    n_init=10,              # N√∫mero de veces que se ejecuta con diferentes centroides
    max_iter=300,           # M√°ximo de iteraciones por ejecuci√≥n
    tol=1e-4,               # Tolerancia para convergencia
    algorithm='lloyd'       # Algoritmo est√°ndar
)

# Entrenar modelo
clusters = kmeans_final.fit_predict(X_simple_scaled)

print(f"‚úÖ Modelo entrenado en {kmeans_final.n_iter_} iteraciones")
print(f"‚úÖ Inercia final: {kmeans_final.inertia_:.2f}")

# Agregar clusters al DataFrame original
df['Cluster'] = clusters

print("\n" + "‚ïê" * 80)
print("DISTRIBUCI√ìN DE CLIENTES POR CLUSTER")
print("‚ïê" * 80)
print(df['Cluster'].value_counts().sort_index())
```

#### Paso 4.2: Analizar Centroides

```python
# Obtener centroides en escala escalada
centroides_scaled = kmeans_final.cluster_centers_

# Revertir escalado para interpretar
centroides_original = scaler_simple.inverse_transform(centroides_scaled)

# Crear DataFrame para mejor visualizaci√≥n
centroides_df = pd.DataFrame(
    centroides_original,
    columns=['Ingresos (k$)', 'Spending Score']
)
centroides_df['Cluster'] = range(K_OPTIMO)

print("\n" + "‚ïê" * 80)
print("CENTROIDES DE CADA CLUSTER (Escala Original)")
print("‚ïê" * 80)
print(centroides_df.to_string(index=False))
```

**Salida esperada** (ejemplo):
```
 Ingresos (k$)  Spending Score  Cluster
         55.30           49.52        0
         86.54           82.13        1
         88.20           17.11        2
         25.73           79.36        3
         26.30           20.91        4
```

#### Paso 4.3: Guardar Modelo (Para Reutilizarlo)

```python
import pickle

# Guardar modelo
with open('results/models/kmeans_final.pkl', 'wb') as f:
    pickle.dump(kmeans_final, f)

# Guardar escalador (importante para predecir nuevos datos)
with open('results/models/scaler.pkl', 'wb') as f:
    pickle.dump(scaler_simple, f)

print("‚úÖ Modelo y escalador guardados")
```

**Para cargar despu√©s**:
```python
# Cargar modelo
with open('results/models/kmeans_final.pkl', 'rb') as f:
    kmeans_loaded = pickle.load(f)

# Predecir nuevo cliente
nuevo_cliente = [[70, 85]]  # Ingresos 70k$, Spending 85
nuevo_cliente_scaled = scaler_simple.transform(nuevo_cliente)
cluster_asignado = kmeans_loaded.predict(nuevo_cliente_scaled)
print(f"Cliente asignado al Cluster {cluster_asignado[0]}")
```

---

### FASE 5: VISUALIZACI√ìN DE RESULTADOS

**Objetivo**: Crear gr√°ficos profesionales para presentar hallazgos.

#### Paso 5.1: Scatter Plot 2D con Clusters

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Paleta de colores profesional
colors = sns.color_palette('husl', K_OPTIMO)

# Crear figura
fig, ax = plt.subplots(figsize=(14, 9))

# Plotear cada cluster
for i in range(K_OPTIMO):
    cluster_data = df[df['Cluster'] == i]
    ax.scatter(
        cluster_data['Annual Income (k$)'],
        cluster_data['Spending Score (1-100)'],
        s=100,                      # Tama√±o de puntos
        c=[colors[i]],              # Color del cluster
        label=f'Cluster {i}',
        alpha=0.6,                  # Transparencia
        edgecolors='black',
        linewidth=0.5
    )

# Plotear centroides
ax.scatter(
    centroides_df['Ingresos (k$)'],
    centroides_df['Spending Score'],
    s=300,                          # M√°s grandes que los puntos
    c='red',
    marker='X',                     # Forma de X
    edgecolors='black',
    linewidth=2,
    label='Centroides',
    zorder=10                       # Dibujarse encima
)

# Configuraci√≥n del gr√°fico
ax.set_xlabel('Ingresos Anuales (k$)', fontsize=14, fontweight='bold')
ax.set_ylabel('Spending Score (1-100)', fontsize=14, fontweight='bold')
ax.set_title('SEGMENTACI√ìN DE CLIENTES - K-MEANS CLUSTERING', 
             fontsize=16, fontweight='bold', pad=20)
ax.legend(fontsize=11, loc='best', framealpha=0.9)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results/figures/02_clustering/segmentacion_2d.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úÖ Gr√°fico de segmentaci√≥n guardado")
```

#### Paso 5.2: Gr√°fico 3D (Si usaste Edad tambi√©n)

```python
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(14, 10))
ax = fig.add_subplot(111, projection='3d')

# Plotear cada cluster
for i in range(K_OPTIMO):
    cluster_data = df[df['Cluster'] == i]
    ax.scatter(
        cluster_data['Age'],
        cluster_data['Annual Income (k$)'],
        cluster_data['Spending Score (1-100)'],
        s=80,
        c=[colors[i]],
        label=f'Cluster {i}',
        alpha=0.6,
        edgecolors='black',
        linewidth=0.5
    )

# Etiquetas
ax.set_xlabel('Edad', fontsize=12, fontweight='bold', labelpad=10)
ax.set_ylabel('Ingresos Anuales (k$)', fontsize=12, fontweight='bold', labelpad=10)
ax.set_zlabel('Spending Score', fontsize=12, fontweight='bold', labelpad=10)
ax.set_title('SEGMENTACI√ìN 3D DE CLIENTES', fontsize=14, fontweight='bold', pad=20)
ax.legend(loc='best')

# Rotar para mejor vista
ax.view_init(elev=20, azim=45)

plt.tight_layout()
plt.savefig('results/figures/02_clustering/segmentacion_3d.png', dpi=300)
plt.show()
```

#### Paso 5.3: An√°lisis Estad√≠stico por Cluster

```python
# Resumen estad√≠stico por cluster
cluster_stats = df.groupby('Cluster').agg({
    'Age': ['mean', 'std', 'min', 'max'],
    'Annual Income (k$)': ['mean', 'std', 'min', 'max'],
    'Spending Score (1-100)': ['mean', 'std', 'min', 'max'],
    'Gender': lambda x: f"{(x=='Female').sum()} F / {(x=='Male').sum()} M",
    'CustomerID': 'count'
}).round(2)

cluster_stats.columns = ['_'.join(col).strip() for col in cluster_stats.columns.values]
cluster_stats.rename(columns={'CustomerID_count': 'Tama√±o'}, inplace=True)

print("\n" + "‚ïê" * 120)
print("RESUMEN ESTAD√çSTICO POR CLUSTER")
print("‚ïê" * 120)
print(cluster_stats)

# Guardar en CSV
cluster_stats.to_csv('results/reports/cluster_statistics.csv')
```

#### Paso 5.4: Heatmap de Caracter√≠sticas por Cluster

```python
# Calcular medias por cluster
cluster_means = df.groupby('Cluster')[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].mean()

# Normalizar para mejor visualizaci√≥n en heatmap
from sklearn.preprocessing import MinMaxScaler
scaler_heatmap = MinMaxScaler()
cluster_means_normalized = scaler_heatmap.fit_transform(cluster_means)
cluster_means_normalized_df = pd.DataFrame(
    cluster_means_normalized,
    index=cluster_means.index,
    columns=cluster_means.columns
)

# Crear heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(cluster_means_normalized_df.T, 
            annot=cluster_means.T.values,  # Mostrar valores originales
            fmt='.1f',
            cmap='YlGnBu',
            cbar_kws={'label': 'Valor Normalizado'},
            linewidths=1,
            linecolor='white')
plt.title('CARACTER√çSTICAS PROMEDIO POR CLUSTER', fontsize=14, fontweight='bold', pad=20)
plt.xlabel('Cluster', fontsize=12, fontweight='bold')
plt.ylabel('Caracter√≠stica', fontsize=12, fontweight='bold')
plt.tight_layout()
plt.savefig('results/figures/02_clustering/heatmap_clusters.png', dpi=300)
plt.show()
```

---

### FASE 6: INTERPRETACI√ìN DE NEGOCIO

**Objetivo**: Convertir clusters matem√°ticos en segmentos de marketing accionables.

#### Paso 6.1: Perfilar Clusters

Basado en los centroides, asigna nombres descriptivos:

```python
# An√°lisis manual basado en centroides
# (Ajusta seg√∫n TUS resultados)

cluster_profiles = {
    0: {
        'nombre': 'Precavidos de Ingresos Medios',
        'descripcion': 'Ingresos medios, spending moderado',
        'caracteristicas': ['Ingresos: 50-60k$', 'Spending: 40-50', 'Edad: Variada'],
        'tama√±o': (df['Cluster'] == 0).sum()
    },
    1: {
        'nombre': 'VIP High Spenders',
        'descripcion': 'Altos ingresos, alto spending',
        'caracteristicas': ['Ingresos: 70-90k$', 'Spending: 75-90', 'Edad: 30-45'],
        'tama√±o': (df['Cluster'] == 1).sum()
    },
    2: {
        'nombre': 'Conservadores de Alto Ingreso',
        'descripcion': 'Altos ingresos, bajo spending',
        'caracteristicas': ['Ingresos: 70-90k$', 'Spending: 10-25', 'Edad: 40-60'],
        'tama√±o': (df['Cluster'] == 2).sum()
    },
    3: {
        'nombre': 'J√≥venes Gastadores',
        'descripcion': 'Bajos ingresos, alto spending',
        'caracteristicas': ['Ingresos: 20-40k$', 'Spending: 70-90', 'Edad: 18-30'],
        'tama√±o': (df['Cluster'] == 3).sum()
    },
    4: {
        'nombre': 'Oportunidad de Crecimiento',
        'descripcion': 'Bajos ingresos, bajo spending',
        'caracteristicas': ['Ingresos: 20-40k$', 'Spending: 10-30', 'Edad: Variada'],
        'tama√±o': (df['Cluster'] == 4).sum()
    }
}

# Agregar nombres al DataFrame
nombre_map = {k: v['nombre'] for k, v in cluster_profiles.items()}
df['Cluster_Nombre'] = df['Cluster'].map(nombre_map)

print("\n" + "‚ïê" * 100)
print("PERFILES DE SEGMENTOS")
print("‚ïê" * 100)
for cluster_id, profile in cluster_profiles.items():
    print(f"\nüè∑Ô∏è  CLUSTER {cluster_id}: {profile['nombre'].upper()}")
    print(f"   Descripci√≥n: {profile['descripcion']}")
    print(f"   Tama√±o: {profile['tama√±o']} clientes ({profile['tama√±o']/len(df)*100:.1f}%)")
    print(f"   Caracter√≠sticas:")
    for char in profile['caracteristicas']:
        print(f"      ‚Ä¢ {char}")
```

#### Paso 6.2: Estrategias de Marketing por Segmento

```python
marketing_strategies = {
    'VIP High Spenders': """
    üéØ ESTRATEGIA:
       ‚Ä¢ Programa de lealtad PREMIUM con beneficios exclusivos
       ‚Ä¢ Acceso temprano a lanzamientos y colecciones limitadas
       ‚Ä¢ Personal shopper y servicios de atenci√≥n personalizada
       ‚Ä¢ Eventos VIP y experiencias √∫nicas
       ‚Ä¢ Comunicaci√≥n directa v√≠a email/WhatsApp
    
    üí∞ INVERSI√ìN: Alta
    üìà POTENCIAL: Muy Alto (son tu base m√°s rentable)
    """,
    
    'J√≥venes Gastadores': """
    üéØ ESTRATEGIA:
       ‚Ä¢ Marketing en redes sociales (Instagram, TikTok)
       ‚Ä¢ Influencer marketing con microinfluencers
       ‚Ä¢ Descuentos por volumen y bundles
       ‚Ä¢ Programa de referidos con incentivos
       ‚Ä¢ Gamificaci√≥n (puntos, badges, desaf√≠os)
    
    üí∞ INVERSI√ìN: Media
    üìà POTENCIAL: Alto (pueden convertirse en VIP futuro)
    """,
    
    'Conservadores de Alto Ingreso': """
    üéØ ESTRATEGIA:
       ‚Ä¢ Demostraci√≥n de VALOR y CALIDAD sobre precio
       ‚Ä¢ Testimonios y casos de √©xito
       ‚Ä¢ Garant√≠as extendidas y pol√≠ticas de devoluci√≥n generosas
       ‚Ä¢ Marketing educativo (webinars, gu√≠as, comparativas)
       ‚Ä¢ Email marketing con contenido de valor
    
    üí∞ INVERSI√ìN: Media-Alta
    üìà POTENCIAL: Alto (tienen el dinero, falta convencerlos)
    """,
    
    'Oportunidad de Crecimiento': """
    üéØ ESTRATEGIA:
       ‚Ä¢ Productos de entrada (loss leaders)
       ‚Ä¢ L√≠nea econ√≥mica de calidad
       ‚Ä¢ Programas de financiamiento y pagos flexibles
       ‚Ä¢ Descuentos por primera compra
       ‚Ä¢ Comunicaci√≥n por WhatsApp y SMS (bajo costo)
    
    üí∞ INVERSI√ìN: Baja
    üìà POTENCIAL: Medio (volumen vs. margen)
    """,
    
    'Precavidos de Ingresos Medios': """
    üéØ ESTRATEGIA:
       ‚Ä¢ Promociones estacionales y flash sales
       ‚Ä¢ Descuentos por cantidad (2x1, 3x2)
       ‚Ä¢ Productos de rango medio con buen value
       ‚Ä¢ Programa de puntos acumulables
       ‚Ä¢ Email marketing con ofertas personalizadas
    
    üí∞ INVERSI√ìN: Media
    üìà POTENCIAL: Medio-Alto (segmento estable)
    """
}

# Imprimir estrategias
print("\n" + "‚ïê" * 100)
print("ESTRATEGIAS DE MARKETING POR SEGMENTO")
print("‚ïê" * 100)
for segmento, estrategia in marketing_strategies.items():
    print(f"\n{segmento.upper()}")
    print("‚îÄ" * 100)
    print(estrategia)
```

#### Paso 6.3: Visualizaci√≥n con Nombres de Negocio

```python
fig, ax = plt.subplots(figsize=(16, 10))

# Plotear con nombres descriptivos
for cluster_id, profile in cluster_profiles.items():
    cluster_data = df[df['Cluster'] == cluster_id]
    ax.scatter(
        cluster_data['Annual Income (k$)'],
        cluster_data['Spending Score (1-100)'],
        s=120,
        label=f"{profile['nombre']} ({profile['tama√±o']})",
        alpha=0.7,
        edgecolors='black',
        linewidth=0.5
    )

ax.set_xlabel('Ingresos Anuales (k$)', fontsize=14, fontweight='bold')
ax.set_ylabel('Spending Score (1-100)', fontsize=14, fontweight='bold')
ax.set_title('SEGMENTACI√ìN DE CLIENTES CON ETIQUETAS DE NEGOCIO', 
             fontsize=16, fontweight='bold', pad=20)
ax.legend(fontsize=11, loc='best', framealpha=0.9, title='Segmentos')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results/figures/03_business/segmentacion_con_nombres.png', dpi=300)
plt.show()
```

---

### FASE 7: EXPORTACI√ìN Y REPORTES

#### Paso 7.1: Exportar Datos con Clusters

```python
# Crear DataFrame final
df_export = df[['CustomerID', 'Gender', 'Age', 
                'Annual Income (k$)', 'Spending Score (1-100)',
                'Cluster', 'Cluster_Nombre']]

# Guardar CSV
df_export.to_csv('data/processed/clientes_segmentados.csv', index=False)
print(f"‚úÖ Datos exportados: {len(df_export)} clientes segmentados")
print(f"   Archivo: data/processed/clientes_segmentados.csv")
```

#### Paso 7.2: Generar Reporte de Texto

```python
from datetime import datetime

with open('results/reports/reporte_segmentacion.txt', 'w', encoding='utf-8') as f:
    f.write("‚ïê" * 100 + "\n")
    f.write("REPORTE DE SEGMENTACI√ìN DE CLIENTES - K-MEANS CLUSTERING\n")
    f.write("‚ïê" * 100 + "\n\n")
    
    f.write(f"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Total de clientes analizados: {len(df)}\n")
    f.write(f"N√∫mero de clusters: {K_OPTIMO}\n")
    f.write(f"Features utilizadas: Ingresos Anuales, Spending Score\n\n")
    
    f.write("‚îÄ" * 100 + "\n")
    f.write("M√âTRICAS DEL MODELO\n")
    f.write("‚îÄ" * 100 + "\n")
    f.write(f"Inercia final: {kmeans_final.inertia_:.2f}\n")
    f.write(f"Iteraciones: {kmeans_final.n_iter_}\n\n")
    
    f.write("‚îÄ" * 100 + "\n")
    f.write("DISTRIBUCI√ìN DE CLIENTES\n")
    f.write("‚îÄ" * 100 + "\n")
    f.write(df['Cluster_Nombre'].value_counts().to_string())
    f.write("\n\n")
    
    f.write("‚îÄ" * 100 + "\n")
    f.write("CENTROIDES POR CLUSTER\n")
    f.write("‚îÄ" * 100 + "\n")
    f.write(centroides_df.to_string(index=False))
    f.write("\n\n")
    
    f.write("‚îÄ" * 100 + "\n")
    f.write("PERFILES DE SEGMENTOS\n")
    f.write("‚îÄ" * 100 + "\n")
    for cluster_id, profile in cluster_profiles.items():
        f.write(f"\nCLUSTER {cluster_id}: {profile['nombre']}\n")
        f.write(f"Descripci√≥n: {profile['descripcion']}\n")
        f.write(f"Tama√±o: {profile['tama√±o']} clientes\n")
        f.write("Caracter√≠sticas:\n")
        for char in profile['caracteristicas']:
            f.write(f"  ‚Ä¢ {char}\n")
    
    f.write("\n" + "‚îÄ" * 100 + "\n")
    f.write("ESTRATEGIAS RECOMENDADAS\n")
    f.write("‚îÄ" * 100 + "\n")
    for segmento, estrategia in marketing_strategies.items():
        f.write(f"\n{segmento}:\n")
        f.write(estrategia + "\n")

print("‚úÖ Reporte generado: results/reports/reporte_segmentacion.txt")
```

---

## üìà INTERPRETACI√ìN DE RESULTADOS

### C√≥mo Leer los Gr√°ficos

#### Scatter Plot 2D
```
Spending Score
     100 ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ              ‚îÇ  1  ‚îÇ ‚Üê VIP High Spenders
         ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      75 ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              
         ‚îÇ  ‚îÇ  3  ‚îÇ              ‚Üê J√≥venes Gastadores
      50 ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ             ‚îÇ  0  ‚îÇ  ‚Üê Precavidos
      25 ‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       0 ‚îÇ  ‚îÇ  4  ‚îÇ             ‚îÇ  2  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Ingresos
            20    40    60    80   100k$
         
         Oportunidad         Conservadores
```

### Interpretaci√≥n de M√©tricas

#### Inercia (WCSS)
- **Valor bajo**: Clusters compactos (bueno)
- **Valor alto**: Clusters dispersos (malo)
- **Comparaci√≥n**: Solo tiene sentido comparar entre diferentes K

#### Silhouette Score
- **0.7 - 1.0**: Estructura fuerte y clara
- **0.5 - 0.7**: Estructura razonable
- **0.25 - 0.5**: Estructura d√©bil
- **< 0.25**: No hay estructura natural

### Validaci√≥n de Resultados

**Preguntas clave**:
1. ‚úÖ ¬øLos clusters tienen sentido de negocio?
2. ‚úÖ ¬øSon accionables las estrategias?
3. ‚úÖ ¬øLos tama√±os de clusters son manejables?
4. ‚úÖ ¬øLas caracter√≠sticas son distintivas?

**Se√±ales de alerta** üö®:
- Cluster con 1-2 clientes (demasiado peque√±o)
- Cluster con >80% de clientes (demasiado general)
- Clusters solapados en visualizaci√≥n
- Caracter√≠sticas casi id√©nticas entre clusters

---





## üîß TROUBLESHOOTING

### Problemas Comunes y Soluciones

#### Problema 1: "ModuleNotFoundError: No module named 'sklearn'"

**Soluci√≥n**:
```bash
pip install scikit-learn
# o
conda install scikit-learn
```

#### Problema 2: El gr√°fico no muestra clusters claros

**Posibles causas**:
- K incorrecto ‚Üí Revisar m√©todo del codo
- Features incorrectas ‚Üí Probar con otras variables
- Datos no escalados ‚Üí Aplicar StandardScaler

**Soluci√≥n**:
```python
# Verificar escalado
print(X_scaled.mean(axis=0))  # Debe ser ~0
print(X_scaled.std(axis=0))   # Debe ser ~1

# Probar diferentes K
for k in [3, 4, 5, 6]:
    # Entrenar y visualizar
```

#### Problema 3: Todos los puntos en un solo cluster

**Causa**: Inercia muy alta, K=1 efectivo

**Soluci√≥n**:
- Aumentar K
- Revisar outliers (eliminarlos con `df = df[df['columna'] < threshold]`)
- Probar con features diferentes

#### Problema 4: Jupyter Notebook no inicia

**Soluci√≥n**:
```bash
# Reinstalar Jupyter
pip uninstall jupyter
pip install jupyter

# O usar JupyterLab
pip install jupyterlab
jupyter lab
```

#### Problema 5: Gr√°ficos no se guardan

**Causa**: Carpeta no existe

**Soluci√≥n**:
```python
import os
os.makedirs('results/figures', exist_ok=True)
```

#### Problema 6: Error "ConvergenceWarning"

**Causa**: El algoritmo no convergi√≥ en max_iter iteraciones

**Soluci√≥n**:
```python
# Aumentar max_iter
kmeans = KMeans(n_clusters=5, max_iter=500, random_state=42)
```

---

## üìö RECURSOS ADICIONALES

### Tutoriales en Video
- [K-Means Explained - StatQuest](https://www.youtube.com/watch?v=4b5d3muPQmA)
- [Machine Learning Course - freeCodeCamp](https://www.youtube.com/watch?v=NWONeJKn6kc)
- [K-Means Clustering en Python - Tech With Tim](https://www.youtube.com/watch?v=EItlUEPCIzM)

### Documentaci√≥n Oficial
- [Scikit-learn K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/index.html)
- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)

### Libros Recomendados
- **"Hands-On Machine Learning"** - Aur√©lien G√©ron
- **"Python Data Science Handbook"** - Jake VanderPlas
- **"Data Science for Business"** - Foster Provost
- **"Introduction to Statistical Learning"** - James, Witten, Hastie, Tibshirani

### Datasets Adicionales para Practicar
- **Kaggle**: https://www.kaggle.com/datasets
  - "Online Retail Dataset"
  - "E-commerce Customer Behavior"
  - "Bank Marketing Dataset"
  - "Credit Card Customers"
  - "Wholesale Customers Dataset"

### Comunidades
- **Stack Overflow** - Para preguntas t√©cnicas
- **Reddit r/MachineLearning** - Discusiones y papers
- **Kaggle Forums** - Compartir proyectos
- **Discord de Python** - Comunidad activa
- **GitHub** - Explorar proyectos open source

### Cursos Online Gratuitos
- **Google Machine Learning Crash Course**
- **Fast.ai Practical Deep Learning**
- **Coursera - Machine Learning by Andrew Ng**
- **edX - Data Science Fundamentals**

---



## üéì CERTIFICADO DE COMPLETION

### Checklist del Proyecto

Marca cuando completes cada fase:

**FASE DE SETUP**
- [ ] Python 3.8+ instalado y verificado
- [ ] Entorno virtual creado y activado
- [ ] Todas las librer√≠as instaladas correctamente
- [ ] Estructura de carpetas creada
- [ ] Dataset Mall_Customers.csv descargado

**FASE DE AN√ÅLISIS**
- [ ] EDA completado con visualizaciones
- [ ] An√°lisis univariado realizado
- [ ] An√°lisis bivariado completado
- [ ] Matriz de correlaci√≥n generada
- [ ] Outliers identificados (si hay)

**FASE DE PREPROCESAMIENTO**
- [ ] Variables categ√≥ricas codificadas
- [ ] Features seleccionadas correctamente
- [ ] Datos escalados con StandardScaler
- [ ] Verificaci√≥n de escalado realizada

**FASE DE MODELADO**
- [ ] M√©todo del Codo implementado
- [ ] K √≥ptimo determinado
- [ ] Silhouette Score calculado (opcional)
- [ ] Modelo K-Means entrenado exitosamente
- [ ] Centroides extra√≠dos y analizados
- [ ] Modelo guardado en pickle

**FASE DE VISUALIZACI√ìN**
- [ ] Gr√°fico 2D de clusters generado
- [ ] Gr√°fico 3D creado (si aplica)
- [ ] Heatmap de caracter√≠sticas producido
- [ ] Estad√≠sticas por cluster calculadas
- [ ] Todos los gr√°ficos guardados en alta resoluci√≥n

**FASE DE NEGOCIO**
- [ ] Perfiles de clusters creados
- [ ] Nombres descriptivos asignados
- [ ] Estrategias de marketing definidas
- [ ] Visualizaci√≥n con nombres de negocio generada
- [ ] Plan de acci√≥n por segmento desarrollado

**FASE DE EXPORTACI√ìN**
- [ ] CSV con clusters exportado
- [ ] Reporte de texto generado
- [ ] Todos los archivos organizados
- [ ] Documentaci√≥n completada

**APLICACI√ìN PR√ÅCTICA**
- [ ] Plan para aplicar a L'Luis desarrollado
- [ ] Estructura de datos de L'Luis definida
- [ ] An√°lisis RFM entendido
- [ ] Estrategias espec√≠ficas para L'Luis creadas

---


## üéØ CASOS DE USO ADICIONALES

### M√°s all√° de Segmentaci√≥n de Clientes

Este mismo enfoque de K-Means se puede aplicar a:

1. **Segmentaci√≥n de Productos**
   - Agrupar productos por caracter√≠sticas
   - Identificar categor√≠as naturales
   - Optimizar inventario

2. **An√°lisis de Comportamiento Web**
   - Segmentar usuarios por navegaci√≥n
   - Identificar patrones de uso
   - Personalizar experiencia

3. **Detecci√≥n de Anomal√≠as**
   - Identificar transacciones fraudulentas
   - Detectar comportamientos inusuales
   - Monitoreo de sistemas

4. **Optimizaci√≥n de Precios**
   - Segmentar productos por elasticidad
   - Identificar nichos de mercado
   - Estrategias de pricing

5. **An√°lisis de Redes Sociales**
   - Identificar comunidades
   - Segmentar influencers
   - Detectar tendencias

---

## üí° TIPS Y MEJORES PR√ÅCTICAS

### Para Obtener Mejores Resultados

1. **Calidad de Datos**
   - ‚ú® Limpia datos antes de clustering
   - ‚ú® Elimina outliers extremos
   - ‚ú® Verifica consistencia

2. **Selecci√≥n de Features**
   - ‚ú® Usa features relevantes al negocio
   - ‚ú® Evita multicolinealidad
   - ‚ú® Considera crear features derivadas

3. **Escalado**
   - ‚ú® SIEMPRE escala antes de K-Means
   - ‚ú® Usa StandardScaler como default
   - ‚ú® Considera RobustScaler si hay outliers

4. **Interpretaci√≥n**
   - ‚ú® Valida clusters con expertos del negocio
   - ‚ú® Nombra clusters descriptivamente
   - ‚ú® Documenta suposiciones

5. **Mantenimiento**
   - ‚ú® Reentrena peri√≥dicamente
   - ‚ú® Monitorea tama√±o de clusters
   - ‚ú® Ajusta K si es necesario

---

**¬°Buena suerte con tu proyecto de segmentaci√≥n! üöÄ**

Si tienes dudas, recuerda: la mejor manera de aprender Machine Learning es **experimentando y equivoc√°ndote**. No tengas miedo de romper cosas, es parte del proceso.

---

*README generado para Carlos - Proyecto de Segmentaci√≥n de Clientes*
*√öltima actualizaci√≥n: Enero 2026*
*Versi√≥n: 1.0*

---

## üì¨ NOTAS FINALES

Este README contiene **TODO** lo necesario para completar el proyecto de segmentaci√≥n de clientes desde cero hasta la implementaci√≥n final. 

**Tiempo estimado de completion**: 8-12 horas para principiantes, 4-6 horas para intermedios.

**Recuerda**: El aprendizaje es un proceso. Si algo no funciona a la primera, revisa el c√≥digo, consulta la documentaci√≥n, y sigue intentando. ¬°Cada error es una oportunidad de aprendizaje!

**¬°√âxito con tu proyecto! üí™**
